{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Normalization Layers: Mathematical Derivations\n\nThis notebook derives the forward and backward passes for two normalization techniques used in transformers:\n\n1. **LayerNorm** (Ba et al., 2016) - Standard in original Transformer\n2. **RMSNorm** (Zhang & Sennrich, 2019) - Used in LLaMA, Gemma, and modern architectures\n\nWe'll derive the math from first principles and verify against our implementations."
  },
  {
   "cell_type": "markdown",
   "source": "## Glossary of Terms\n\n| Term | Definition |\n|------|------------|\n| **Feature dimension** | The last axis of a tensor representing the learned attributes of each token. In a tensor of shape `(batch, sequence, features)`, the feature dimension has size `features` (often called `d_model`, typically 512-4096). Each position along this axis represents a different learned characteristic. |\n| **Batch dimension** | The first axis of a tensor, representing independent samples processed together for efficiency. Shape: `(batch, ...)`. |\n| **Sequence dimension** | The second axis in transformer tensors, representing positions in the input sequence (e.g., words in a sentence). Shape: `(batch, sequence, features)`. |\n| **Normalization** | Rescaling values to have specific statistical properties (e.g., zero mean, unit variance). Stabilizes training by preventing activations from growing unboundedly. |\n| **Internal covariate shift** | The phenomenon where the distribution of layer inputs changes during training as earlier layers update their weights, making optimization harder. |\n| **Learnable parameters** | Weights ($\\gamma$, $\\beta$) that are updated during training via gradient descent, allowing the network to adapt the normalization. |\n| **Upstream gradient** | The gradient $\\frac{\\partial L}{\\partial y}$ flowing backward from later layers. We use this to compute gradients for earlier layers. |\n| **Epsilon ($\\epsilon$)** | A small constant (e.g., $10^{-5}$) added to denominators to prevent division by zero when variance is near zero. |\n| **Affine transformation** | A linear transformation followed by a translation: $y = \\gamma x + \\beta$. The scale ($\\gamma$) and shift ($\\beta$) in normalization layers. |\n| **RMS (Root Mean Square)** | $\\sqrt{\\frac{1}{n}\\sum x_i^2}$ - the quadratic mean of values. Unlike standard deviation, it doesn't subtract the mean first. |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Formulas and Theorems\n\n### Layer Normalization\n\n| Formula | Description |\n|---------|-------------|\n| $\\mu = \\frac{1}{d} \\sum_{i=1}^{d} x_i$ | Mean over feature dimension |\n| $\\sigma^2 = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2$ | Variance over feature dimension |\n| $\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$ | Normalized features |\n| $y = \\gamma \\odot \\hat{x} + \\beta$ | LayerNorm output (with learnable $\\gamma, \\beta$) |\n| $\\frac{\\partial L}{\\partial \\gamma_i} = \\sum \\bar{y}_i \\hat{x}_i$ | Gradient w.r.t. scale parameter |\n| $\\frac{\\partial L}{\\partial \\beta_i} = \\sum \\bar{y}_i$ | Gradient w.r.t. shift parameter |\n| $\\frac{\\partial L}{\\partial x} = \\frac{1}{\\sigma}\\left( \\bar{\\hat{x}} - \\overline{\\bar{\\hat{x}}} - \\hat{x} \\cdot \\overline{\\bar{\\hat{x}} \\odot \\hat{x}} \\right)$ | Backward pass (where $\\overline{(\\cdot)}$ denotes mean) |\n\n### RMS Normalization\n\n| Formula | Description |\n|---------|-------------|\n| $\\text{RMS}(x) = \\sqrt{\\frac{1}{d}\\sum_{i=1}^{d} x_i^2 + \\epsilon}$ | Root Mean Square |\n| $\\hat{x} = \\frac{x}{\\text{RMS}(x)}$ | RMS-normalized features |\n| $y = \\gamma \\odot \\hat{x}$ | RMSNorm output (no $\\beta$ parameter) |\n| $\\frac{\\partial L}{\\partial \\gamma_i} = \\sum \\bar{y}_i \\hat{x}_i$ | Gradient w.r.t. scale parameter |\n| $\\frac{\\partial L}{\\partial x} = \\frac{1}{r}\\left( \\bar{\\hat{x}} - \\hat{x} \\cdot \\overline{\\bar{\\hat{x}} \\odot \\hat{x}} \\right)$ | Backward pass (simpler than LayerNorm) |\n\n### Key Identities\n\n| Identity | Description |\n|----------|-------------|\n| $\\text{Var}(x) = \\text{E}[x^2] - \\text{E}[x]^2$ | Variance decomposition |\n| $\\text{RMS}(x)^2 = \\text{Var}(x) + \\mu^2$ | Relationship between RMS and variance |\n| $\\frac{\\partial}{\\partial x_i} \\sqrt{f(x)} = \\frac{f'(x_i)}{2\\sqrt{f(x)}}$ | Derivative of square root |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Prerequisites\n\nThis notebook assumes familiarity with:\n\n### 1. Mean and Variance\n\nFor a vector $\\mathbf{x} = [x_1, x_2, ..., x_d]$:\n\n**Mean (Expected Value):**\n$$\\mu = \\text{E}[x] = \\frac{1}{d} \\sum_{i=1}^{d} x_i$$\n\n**Variance:** The average squared deviation from the mean:\n$$\\text{Var}(x) = \\sigma^2 = \\text{E}[(x - \\mu)^2] = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2$$\n\n**Standard Deviation:** $\\sigma = \\sqrt{\\text{Var}(x)}$\n\n**Alternative variance formula:** $\\text{Var}(x) = \\text{E}[x^2] - \\text{E}[x]^2$\n\n### 2. The Chain Rule for Multivariate Functions\n\nWhen a variable $x_i$ affects the output through multiple paths, we sum the gradients from each path:\n\n$$\\frac{\\partial L}{\\partial x_i} = \\sum_{\\text{all paths}} \\frac{\\partial L}{\\partial (\\text{intermediate})} \\cdot \\frac{\\partial (\\text{intermediate})}{\\partial x_i}$$\n\nFor normalization, $x_i$ affects the output through:\n1. The direct path: $\\hat{x}_i$ depends on $x_i$\n2. The mean: $\\mu$ depends on all $x_j$\n3. The variance/RMS: $\\sigma$ or $r$ depends on all $x_j$\n\n### 3. Element-wise Operations\n\nWe use $\\odot$ to denote element-wise (Hadamard) multiplication:\n$$(a \\odot b)_i = a_i \\cdot b_i$$\n\nThis is different from matrix multiplication or dot products.\n\n### 4. Gradient Notation\n\nWe use bar notation for gradients:\n- $\\bar{y} = \\frac{\\partial L}{\\partial y}$ (upstream gradient)\n- $\\bar{x} = \\frac{\\partial L}{\\partial x}$ (gradient we're computing)\n- $\\bar{\\hat{x}} = \\bar{y} \\odot \\gamma$ (gradient into the normalized value)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from ai_comps.normalization import LayerNorm, RMSNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Layer Normalization\n",
    "\n",
    "### 1.1 Motivation\n",
    "\n",
    "Deep networks suffer from **internal covariate shift**: the distribution of layer inputs changes during training as earlier layers update. LayerNorm stabilizes training by normalizing activations to zero mean and unit variance *within each sample*.\n",
    "\n",
    "Unlike BatchNorm (which normalizes across the batch dimension), LayerNorm normalizes across the feature dimension, making it suitable for:\n",
    "- Variable-length sequences (no batch statistics needed)\n",
    "- Small batch sizes\n",
    "- Autoregressive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Forward Pass\n",
    "\n",
    "Given input $\\mathbf{x} \\in \\mathbb{R}^d$ (a single token's features), LayerNorm computes:\n",
    "\n",
    "**Step 1: Compute mean**\n",
    "$$\\mu = \\frac{1}{d} \\sum_{i=1}^{d} x_i$$\n",
    "\n",
    "**Step 2: Compute variance**\n",
    "$$\\sigma^2 = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2$$\n",
    "\n",
    "**Step 3: Normalize**\n",
    "$$\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "\n",
    "where $\\epsilon \\approx 10^{-5}$ prevents division by zero.\n",
    "\n",
    "**Step 4: Scale and shift (learnable)**\n",
    "$$y_i = \\gamma_i \\hat{x}_i + \\beta_i$$\n",
    "\n",
    "The parameters $\\gamma, \\beta \\in \\mathbb{R}^d$ allow the network to learn to \"undo\" the normalization if beneficial.\n",
    "\n",
    "**Compact form:**\n",
    "$$y = \\gamma \\odot \\frac{x - \\mu}{\\sigma} + \\beta$$\n",
    "\n",
    "where $\\sigma = \\sqrt{\\sigma^2 + \\epsilon}$ and $\\odot$ denotes element-wise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify forward pass\n",
    "np.random.seed(42)\n",
    "d = 4\n",
    "x = np.random.randn(d).astype(np.float32)\n",
    "\n",
    "# Manual computation\n",
    "eps = 1e-5\n",
    "mu = x.mean()\n",
    "var = ((x - mu) ** 2).mean()\n",
    "sigma = np.sqrt(var + eps)\n",
    "x_hat = (x - mu) / sigma\n",
    "\n",
    "print(f\"Input x:     {x}\")\n",
    "print(f\"Mean μ:      {mu:.6f}\")\n",
    "print(f\"Variance σ²: {var:.6f}\")\n",
    "print(f\"Std σ:       {sigma:.6f}\")\n",
    "print(f\"Normalized:  {x_hat}\")\n",
    "print(f\"\\nVerify: mean(x_hat) = {x_hat.mean():.6f} (should be ≈0)\")\n",
    "print(f\"Verify: var(x_hat)  = {x_hat.var():.6f} (should be ≈1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Backward Pass Derivation\n",
    "\n",
    "This is where it gets interesting. Given upstream gradient $\\frac{\\partial L}{\\partial y}$ (denoted $\\bar{y}$), we need:\n",
    "\n",
    "1. $\\frac{\\partial L}{\\partial \\gamma}$ and $\\frac{\\partial L}{\\partial \\beta}$ (parameter gradients)\n",
    "2. $\\frac{\\partial L}{\\partial x}$ (gradient to propagate backward)\n",
    "\n",
    "#### Parameter gradients (easy)\n",
    "\n",
    "From $y_i = \\gamma_i \\hat{x}_i + \\beta_i$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\gamma_i} = \\sum_{\\text{batch, seq}} \\bar{y}_i \\cdot \\hat{x}_i$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\beta_i} = \\sum_{\\text{batch, seq}} \\bar{y}_i$$\n",
    "\n",
    "#### Input gradient (tricky)\n",
    "\n",
    "The challenge: $\\hat{x}_i$ depends on ALL $x_j$ through $\\mu$ and $\\sigma$.\n",
    "\n",
    "Let's define intermediate quantities:\n",
    "- $\\bar{\\hat{x}}_i = \\bar{y}_i \\cdot \\gamma_i$ (gradient into normalized value)\n",
    "\n",
    "Using the chain rule through $\\hat{x}_i = \\frac{x_i - \\mu}{\\sigma}$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial \\hat{x}_i}\\frac{\\partial \\hat{x}_i}{\\partial x_i} + \\sum_j \\frac{\\partial L}{\\partial \\hat{x}_j}\\frac{\\partial \\hat{x}_j}{\\partial \\mu}\\frac{\\partial \\mu}{\\partial x_i} + \\sum_j \\frac{\\partial L}{\\partial \\hat{x}_j}\\frac{\\partial \\hat{x}_j}{\\partial \\sigma}\\frac{\\partial \\sigma}{\\partial x_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing each term\n",
    "\n",
    "**Direct term:** $\\frac{\\partial \\hat{x}_i}{\\partial x_i} = \\frac{1}{\\sigma}$\n",
    "\n",
    "**Through mean:** $\\frac{\\partial \\mu}{\\partial x_i} = \\frac{1}{d}$, and $\\frac{\\partial \\hat{x}_j}{\\partial \\mu} = -\\frac{1}{\\sigma}$\n",
    "\n",
    "**Through variance:** This requires care. Let $v = \\sigma^2 = \\frac{1}{d}\\sum_j(x_j - \\mu)^2$\n",
    "\n",
    "$$\\frac{\\partial v}{\\partial x_i} = \\frac{2(x_i - \\mu)}{d}$$\n",
    "\n",
    "$$\\frac{\\partial \\sigma}{\\partial v} = \\frac{1}{2\\sigma}$$\n",
    "\n",
    "$$\\frac{\\partial \\hat{x}_j}{\\partial \\sigma} = -\\frac{x_j - \\mu}{\\sigma^2} = -\\frac{\\hat{x}_j}{\\sigma}$$\n",
    "\n",
    "#### Putting it together\n",
    "\n",
    "After algebra (see Ba et al., 2016), the clean form is:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x_i} = \\frac{1}{\\sigma}\\left( \\bar{\\hat{x}}_i - \\frac{1}{d}\\sum_j \\bar{\\hat{x}}_j - \\frac{\\hat{x}_i}{d}\\sum_j \\bar{\\hat{x}}_j \\hat{x}_j \\right)$$\n",
    "\n",
    "In vector form:\n",
    "\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial x} = \\frac{1}{\\sigma}\\left( \\bar{\\hat{x}} - \\text{mean}(\\bar{\\hat{x}}) - \\hat{x} \\cdot \\text{mean}(\\bar{\\hat{x}} \\odot \\hat{x}) \\right)}$$\n",
    "\n",
    "where $\\bar{\\hat{x}} = \\bar{y} \\odot \\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify backward pass with numerical gradient check\n",
    "def layernorm_forward(x, gamma, beta, eps=1e-5):\n",
    "    mu = x.mean()\n",
    "    var = ((x - mu) ** 2).mean()\n",
    "    sigma = np.sqrt(var + eps)\n",
    "    x_hat = (x - mu) / sigma\n",
    "    y = gamma * x_hat + beta\n",
    "    return y, x_hat, sigma\n",
    "\n",
    "def layernorm_backward(dy, x_hat, sigma, gamma):\n",
    "    \"\"\"Analytical backward pass.\"\"\"\n",
    "    d = dy.shape[-1]\n",
    "    dx_hat = dy * gamma\n",
    "    \n",
    "    # The key formula\n",
    "    m1 = dx_hat.mean()                    # mean(dx_hat)\n",
    "    m2 = (dx_hat * x_hat).mean()          # mean(dx_hat * x_hat)\n",
    "    dx = (dx_hat - m1 - x_hat * m2) / sigma\n",
    "    \n",
    "    dgamma = (dy * x_hat).sum()\n",
    "    dbeta = dy.sum()\n",
    "    return dx, dgamma, dbeta\n",
    "\n",
    "# Numerical gradient check\n",
    "x = np.random.randn(4).astype(np.float64)  # float64 for precision\n",
    "gamma = np.ones(4, dtype=np.float64)\n",
    "beta = np.zeros(4, dtype=np.float64)\n",
    "\n",
    "y, x_hat, sigma = layernorm_forward(x, gamma, beta)\n",
    "dy = np.random.randn(4).astype(np.float64)  # upstream gradient\n",
    "\n",
    "# Analytical gradient\n",
    "dx_analytical, _, _ = layernorm_backward(dy, x_hat, sigma, gamma)\n",
    "\n",
    "# Numerical gradient\n",
    "eps_num = 1e-5\n",
    "dx_numerical = np.zeros_like(x)\n",
    "for i in range(len(x)):\n",
    "    x_plus = x.copy(); x_plus[i] += eps_num\n",
    "    x_minus = x.copy(); x_minus[i] -= eps_num\n",
    "    y_plus, _, _ = layernorm_forward(x_plus, gamma, beta)\n",
    "    y_minus, _, _ = layernorm_forward(x_minus, gamma, beta)\n",
    "    dx_numerical[i] = ((y_plus - y_minus) * dy).sum() / (2 * eps_num)\n",
    "\n",
    "print(\"Analytical gradient: \", dx_analytical)\n",
    "print(\"Numerical gradient:  \", dx_numerical)\n",
    "print(f\"Max difference: {np.abs(dx_analytical - dx_numerical).max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: RMS Normalization\n",
    "\n",
    "### 2.1 Motivation\n",
    "\n",
    "RMSNorm (Zhang & Sennrich, 2019) simplifies LayerNorm by removing the mean-centering step. The hypothesis: **re-centering is not essential; re-scaling is what matters.**\n",
    "\n",
    "Benefits:\n",
    "- ~10-15% faster (no mean computation)\n",
    "- Fewer operations in backward pass\n",
    "- Empirically works just as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Forward Pass\n",
    "\n",
    "Given input $\\mathbf{x} \\in \\mathbb{R}^d$:\n",
    "\n",
    "**Step 1: Compute RMS (Root Mean Square)**\n",
    "$$\\text{RMS}(x) = \\sqrt{\\frac{1}{d}\\sum_{i=1}^{d} x_i^2 + \\epsilon}$$\n",
    "\n",
    "**Step 2: Normalize**\n",
    "$$\\hat{x}_i = \\frac{x_i}{\\text{RMS}(x)}$$\n",
    "\n",
    "**Step 3: Scale (learnable)**\n",
    "$$y_i = \\gamma_i \\hat{x}_i$$\n",
    "\n",
    "Note: No $\\beta$ (shift) parameter! The mean is not centered, so a shift would just be absorbed.\n",
    "\n",
    "**Compact form:**\n",
    "$$y = \\gamma \\odot \\frac{x}{\\text{RMS}(x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify RMSNorm forward\n",
    "x = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n",
    "eps = 1e-6\n",
    "\n",
    "# Manual computation\n",
    "rms = np.sqrt((x ** 2).mean() + eps)\n",
    "x_hat = x / rms\n",
    "\n",
    "print(f\"Input x:    {x}\")\n",
    "print(f\"RMS:        {rms:.6f}\")\n",
    "print(f\"Normalized: {x_hat}\")\n",
    "print(f\"\\nNote: mean(x_hat) = {x_hat.mean():.4f} (NOT zero, unlike LayerNorm)\")\n",
    "print(f\"But: RMS(x_hat) = {np.sqrt((x_hat**2).mean()):.4f} (IS one)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Backward Pass Derivation\n",
    "\n",
    "The backward pass is simpler than LayerNorm because there's no mean to track.\n",
    "\n",
    "#### Parameter gradient\n",
    "\n",
    "From $y_i = \\gamma_i \\hat{x}_i$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\gamma_i} = \\sum_{\\text{batch, seq}} \\bar{y}_i \\cdot \\hat{x}_i$$\n",
    "\n",
    "#### Input gradient\n",
    "\n",
    "Let $r = \\text{RMS}(x) = \\sqrt{\\frac{1}{d}\\sum_j x_j^2 + \\epsilon}$\n",
    "\n",
    "Then $\\hat{x}_i = \\frac{x_i}{r}$ and $y_i = \\gamma_i \\hat{x}_i$.\n",
    "\n",
    "The gradient $\\bar{x}_i = \\frac{\\partial L}{\\partial x_i}$ has two paths:\n",
    "1. Direct: through $\\hat{x}_i = x_i / r$\n",
    "2. Indirect: through $r$ which depends on all $x_j$\n",
    "\n",
    "**Direct path:**\n",
    "$$\\frac{\\partial \\hat{x}_i}{\\partial x_i}\\bigg|_{r\\text{ fixed}} = \\frac{1}{r}$$\n",
    "\n",
    "**Indirect path through $r$:**\n",
    "$$\\frac{\\partial r}{\\partial x_i} = \\frac{x_i}{d \\cdot r}$$\n",
    "\n",
    "$$\\frac{\\partial \\hat{x}_j}{\\partial r} = -\\frac{x_j}{r^2} = -\\frac{\\hat{x}_j}{r}$$\n",
    "\n",
    "Combining with chain rule:\n",
    "$$\\frac{\\partial L}{\\partial x_i} = \\frac{\\bar{\\hat{x}}_i}{r} - \\frac{x_i}{d \\cdot r^2} \\sum_j \\bar{\\hat{x}}_j \\hat{x}_j$$\n",
    "\n",
    "where $\\bar{\\hat{x}}_i = \\bar{y}_i \\gamma_i$.\n",
    "\n",
    "**Simplified form:**\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial x} = \\frac{1}{r}\\left( \\bar{\\hat{x}} - \\hat{x} \\cdot \\text{mean}(\\bar{\\hat{x}} \\odot \\hat{x}) \\right)}$$\n",
    "\n",
    "Compare to LayerNorm's backward:\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{1}{\\sigma}\\left( \\bar{\\hat{x}} - \\text{mean}(\\bar{\\hat{x}}) - \\hat{x} \\cdot \\text{mean}(\\bar{\\hat{x}} \\odot \\hat{x}) \\right)$$\n",
    "\n",
    "RMSNorm is missing the $- \\text{mean}(\\bar{\\hat{x}})$ term (because no mean-centering in forward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify RMSNorm backward pass\n",
    "def rmsnorm_forward(x, gamma, eps=1e-6):\n",
    "    rms = np.sqrt((x ** 2).mean() + eps)\n",
    "    x_hat = x / rms\n",
    "    y = gamma * x_hat\n",
    "    return y, x_hat, rms\n",
    "\n",
    "def rmsnorm_backward(dy, x_hat, rms, gamma):\n",
    "    \"\"\"Analytical backward pass.\"\"\"\n",
    "    dx_hat = dy * gamma\n",
    "    \n",
    "    # The key formula (simpler than LayerNorm!)\n",
    "    m = (dx_hat * x_hat).mean()  # mean(dx_hat * x_hat)\n",
    "    dx = (dx_hat - x_hat * m) / rms\n",
    "    \n",
    "    dgamma = (dy * x_hat).sum()\n",
    "    return dx, dgamma\n",
    "\n",
    "# Numerical gradient check\n",
    "x = np.random.randn(4).astype(np.float64)\n",
    "gamma = np.ones(4, dtype=np.float64)\n",
    "\n",
    "y, x_hat, rms = rmsnorm_forward(x, gamma)\n",
    "dy = np.random.randn(4).astype(np.float64)\n",
    "\n",
    "# Analytical gradient\n",
    "dx_analytical, _ = rmsnorm_backward(dy, x_hat, rms, gamma)\n",
    "\n",
    "# Numerical gradient\n",
    "eps_num = 1e-5\n",
    "dx_numerical = np.zeros_like(x)\n",
    "for i in range(len(x)):\n",
    "    x_plus = x.copy(); x_plus[i] += eps_num\n",
    "    x_minus = x.copy(); x_minus[i] -= eps_num\n",
    "    y_plus, _, _ = rmsnorm_forward(x_plus, gamma)\n",
    "    y_minus, _, _ = rmsnorm_forward(x_minus, gamma)\n",
    "    dx_numerical[i] = ((y_plus - y_minus) * dy).sum() / (2 * eps_num)\n",
    "\n",
    "print(\"Analytical gradient: \", dx_analytical)\n",
    "print(\"Numerical gradient:  \", dx_numerical)\n",
    "print(f\"Max difference: {np.abs(dx_analytical - dx_numerical).max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Comparison and Insights\n",
    "\n",
    "### Why does RMSNorm work without centering?\n",
    "\n",
    "LayerNorm's centering ($x - \\mu$) removes the DC component of the signal. But:\n",
    "\n",
    "1. **Transformers use residual connections**: $y = x + f(x)$. Even if $f(x)$ has non-zero mean, the residual adds it back.\n",
    "\n",
    "2. **Learnable $\\gamma$ can compensate**: If mean matters, the network can learn to encode it in the scale.\n",
    "\n",
    "3. **Empirically equivalent**: Zhang & Sennrich (2019) showed no accuracy loss on translation tasks.\n",
    "\n",
    "### Computational comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count operations\n",
    "d = 512  # typical model dimension\n",
    "\n",
    "print(\"Forward pass operations per vector:\")\n",
    "print(f\"  LayerNorm: {d} (sum for μ) + {d} (sub) + {d} (sq) + {d} (sum for σ²) + {d} (div) + {d} (mul) + {d} (add) = ~{7*d}\")\n",
    "print(f\"  RMSNorm:   {d} (sq) + {d} (sum) + {d} (div) + {d} (mul) = ~{4*d}\")\n",
    "print(f\"\\nRMSNorm is ~{7/4:.1f}x fewer operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient flow comparison\n",
    "\n",
    "Both normalizations ensure gradients don't vanish/explode by keeping activations bounded. But their Jacobians differ:\n",
    "\n",
    "**LayerNorm Jacobian** (how $\\partial y_i / \\partial x_j$ varies):\n",
    "- Diagonal: $\\frac{\\gamma_i}{\\sigma}$\n",
    "- Off-diagonal: $-\\frac{\\gamma_i}{d\\sigma}(1 + \\hat{x}_i\\hat{x}_j)$\n",
    "\n",
    "**RMSNorm Jacobian:**\n",
    "- Diagonal: $\\frac{\\gamma_i}{r}(1 - \\frac{\\hat{x}_i^2}{d})$\n",
    "- Off-diagonal: $-\\frac{\\gamma_i \\hat{x}_i \\hat{x}_j}{d \\cdot r}$\n",
    "\n",
    "Both have full-rank Jacobians (good for gradient flow), but RMSNorm's is slightly sparser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Verify Against Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our actual implementation\n",
    "np.random.seed(123)\n",
    "x = np.random.randn(2, 4, 8).astype(np.float32)  # (batch, seq, dim)\n",
    "dy = np.random.randn(2, 4, 8).astype(np.float32)\n",
    "\n",
    "# LayerNorm\n",
    "ln = LayerNorm(d_model=8)\n",
    "y_ln = ln.forward(x)\n",
    "dx_ln = ln.backward(dy)\n",
    "\n",
    "print(\"LayerNorm:\")\n",
    "print(f\"  Input shape:  {x.shape}\")\n",
    "print(f\"  Output shape: {y_ln.shape}\")\n",
    "print(f\"  Output mean:  {y_ln.mean(axis=-1)[0,0]:.6f} (should be ≈0)\")\n",
    "print(f\"  Output var:   {y_ln.var(axis=-1)[0,0]:.6f} (should be ≈1)\")\n",
    "\n",
    "# RMSNorm\n",
    "rn = RMSNorm(d_model=8)\n",
    "y_rn = rn.forward(x)\n",
    "dx_rn = rn.backward(dy)\n",
    "\n",
    "print(\"\\nRMSNorm:\")\n",
    "print(f\"  Input shape:  {x.shape}\")\n",
    "print(f\"  Output shape: {y_rn.shape}\")\n",
    "print(f\"  Output RMS:   {np.sqrt((y_rn**2).mean(axis=-1))[0,0]:.6f} (should be ≈1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Aspect | LayerNorm | RMSNorm |\n",
    "|--------|-----------|----------|\n",
    "| Forward | $\\gamma \\odot \\frac{x-\\mu}{\\sigma} + \\beta$ | $\\gamma \\odot \\frac{x}{\\text{RMS}(x)}$ |\n",
    "| Parameters | $\\gamma, \\beta$ | $\\gamma$ only |\n",
    "| Centers mean? | Yes | No |\n",
    "| Normalizes scale? | Yes (to unit var) | Yes (to unit RMS) |\n",
    "| Backward complexity | Higher | Lower |\n",
    "| Used in | BERT, GPT-2/3, Original Transformer | LLaMA, Gemma, T5 |\n",
    "\n",
    "**Key insight**: The mean-centering in LayerNorm is mathematically elegant but practically unnecessary in deep residual networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}