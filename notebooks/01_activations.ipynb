{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions: ReLU and GELU\n",
    "\n",
    "This notebook derives the forward and backward passes for activation functions used in transformers.\n",
    "\n",
    "---\n",
    "\n",
    "## Formulas and Theorems Used\n",
    "\n",
    "### Definitions\n",
    "\n",
    "| Name | Formula | Used For |\n",
    "|------|---------|----------|\n",
    "| ReLU | $\\text{ReLU}(x) = \\max(0, x)$ | Forward pass |\n",
    "| GELU | $\\text{GELU}(x) = x \\cdot \\Phi(x)$ | Forward pass |\n",
    "| Normal CDF | $\\Phi(x) = \\frac{1}{2}\\left[1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right]$ | GELU definition |\n",
    "| Error function | $\\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}}\\int_0^x e^{-t^2}dt$ | Normal CDF |\n",
    "| Tanh | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | GELU approximation |\n",
    "\n",
    "### Calculus Rules\n",
    "\n",
    "| Rule | Formula | Used For |\n",
    "|------|---------|----------|\n",
    "| Chain Rule | $\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)$ | All backward passes |\n",
    "| Product Rule | $\\frac{d}{dx}[f(x) \\cdot g(x)] = f'(x)g(x) + f(x)g'(x)$ | GELU backward |\n",
    "| Tanh derivative | $\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x) = \\text{sech}^2(x)$ | GELU backward |\n",
    "\n",
    "### Key GELU Formulas\n",
    "\n",
    "| Formula | Expression |\n",
    "|---------|------------|\n",
    "| GELU (tanh approx) | $\\text{GELU}(x) = \\frac{x}{2}\\left[1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}}\\left(x + 0.044715 x^3\\right)\\right)\\right]$ |\n",
    "| GELU derivative | $\\text{GELU}'(x) = \\frac{1}{2}(1 + \\tanh u) + \\frac{x}{2}(1 - \\tanh^2 u) \\cdot \\frac{du}{dx}$ |\n",
    "| where | $u = \\sqrt{\\frac{2}{\\pi}}(x + 0.044715x^3)$, $\\frac{du}{dx} = \\sqrt{\\frac{2}{\\pi}}(1 + 0.134145x^2)$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Glossary of Terms\n\n| Term | Definition |\n|------|------------|\n| **Activation function** | A nonlinear function applied element-wise to layer outputs. Without activations, a neural network would just be a linear transformation, unable to learn complex patterns. |\n| **Nonlinearity** | A function where $f(ax + by) \\neq af(x) + bf(y)$. Essential for neural networks to approximate arbitrary functions. |\n| **Forward pass** | Computing the output of a layer or network given an input. Propagates data from input to output. |\n| **Backward pass** | Computing gradients of the loss with respect to parameters. Propagates error signals from output back to input (backpropagation). |\n| **Gradient** | The derivative of the loss function with respect to a parameter. Points in the direction of steepest increase; we move opposite to update weights. |\n| **Upstream gradient** | $\\frac{\\partial L}{\\partial y}$ — the gradient flowing backward from later layers in the network. We multiply this by the local gradient to get the gradient for earlier layers. |\n| **Vanishing gradient** | When gradients become very small (near zero) as they propagate backward through many layers, making early layers learn very slowly or not at all. |\n| **Dead neuron** | A neuron (in ReLU networks) that always outputs zero because its input is always negative. Since ReLU'(x) = 0 for x < 0, the gradient is always zero and the neuron never updates. |\n| **CDF (Cumulative Distribution Function)** | $\\Phi(x) = P(X \\leq x)$ — the probability that a random variable is less than or equal to $x$. For GELU, we use the standard normal CDF. |\n| **Error function (erf)** | A special function related to the normal CDF: $\\Phi(x) = \\frac{1}{2}[1 + \\text{erf}(x/\\sqrt{2})]$. Has no closed-form solution, requires numerical computation. |\n| **Numerical stability** | Implementing formulas to avoid overflow/underflow or loss of precision. E.g., using $x - \\max(x)$ before $\\exp$ to prevent overflow. |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites: Math Skills Review\n",
    "\n",
    "### The Chain Rule (Critical for Backpropagation)\n",
    "\n",
    "The chain rule tells us how to differentiate composite functions. If $y = f(g(x))$, then:\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "**Intuition**: If $g$ changes by a small amount $\\Delta g$, and $f$ changes by $\\Delta f$ in response, then the total change in $y$ when $x$ changes is the product of these rates.\n",
    "\n",
    "**Example**: Let $y = (3x + 2)^2$. Here $g(x) = 3x + 2$ and $f(g) = g^2$.\n",
    "- $\\frac{dg}{dx} = 3$\n",
    "- $\\frac{df}{dg} = 2g$\n",
    "- $\\frac{dy}{dx} = 2g \\cdot 3 = 6(3x + 2)$\n",
    "\n",
    "### The Product Rule\n",
    "\n",
    "For $y = f(x) \\cdot g(x)$:\n",
    "\n",
    "$$\\frac{dy}{dx} = f'(x) \\cdot g(x) + f(x) \\cdot g'(x)$$\n",
    "\n",
    "**Intuition**: Both $f$ and $g$ are changing. The total change in the product comes from: (change in $f$) × (value of $g$) + (value of $f$) × (change in $g$).\n",
    "\n",
    "**Example**: Let $y = x \\cdot \\sin(x)$.\n",
    "- $f(x) = x$, $f'(x) = 1$\n",
    "- $g(x) = \\sin(x)$, $g'(x) = \\cos(x)$\n",
    "- $\\frac{dy}{dx} = 1 \\cdot \\sin(x) + x \\cdot \\cos(x) = \\sin(x) + x\\cos(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic Functions\n",
    "\n",
    "The hyperbolic tangent (tanh) is defined as:\n",
    "\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = \\frac{\\sinh(x)}{\\cosh(x)}$$\n",
    "\n",
    "**Key properties**:\n",
    "- Range: $(-1, 1)$ — always between -1 and 1\n",
    "- $\\tanh(0) = 0$\n",
    "- Odd function: $\\tanh(-x) = -\\tanh(x)$\n",
    "- Asymptotes: $\\lim_{x \\to \\infty} \\tanh(x) = 1$, $\\lim_{x \\to -\\infty} \\tanh(x) = -1$\n",
    "\n",
    "**Derivative** (important!):\n",
    "$$\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x) = \\text{sech}^2(x)$$\n",
    "\n",
    "This is called \"sech squared\" (hyperbolic secant squared). The key insight: **if you already computed $\\tanh(x)$, you can get the derivative cheaply** by just squaring and subtracting from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from ai_comps.activations import relu, relu_backward, gelu, gelu_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tanh and its derivative\n",
    "x = np.linspace(-4, 4, 200)\n",
    "tanh_x = np.tanh(x)\n",
    "tanh_deriv = 1 - tanh_x**2  # This is sech^2(x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(x, tanh_x, label='tanh(x)', linewidth=2)\n",
    "ax.plot(x, tanh_deriv, label=\"tanh'(x) = 1 - tanh²(x)\", linewidth=2)\n",
    "ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "ax.axhline(y=1, color='k', linestyle='--', alpha=0.3)\n",
    "ax.axhline(y=-1, color='k', linestyle='--', alpha=0.3)\n",
    "ax.set_xlabel('x')\n",
    "ax.legend()\n",
    "ax.set_title('Tanh and its Derivative')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: tanh'(x) is largest at x=0 and decays to 0 for large |x|\")\n",
    "print(\"This is why tanh has 'vanishing gradient' problems for large inputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Normal Distribution and Its CDF\n",
    "\n",
    "The standard normal distribution has PDF:\n",
    "$$\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}$$\n",
    "\n",
    "The CDF (cumulative distribution function) is:\n",
    "$$\\Phi(x) = P(X \\leq x) = \\int_{-\\infty}^{x} \\phi(t) \\, dt$$\n",
    "\n",
    "**Key properties**:\n",
    "- $\\Phi(-\\infty) = 0$, $\\Phi(\\infty) = 1$\n",
    "- $\\Phi(0) = 0.5$\n",
    "- $\\Phi(-x) = 1 - \\Phi(x)$ (symmetry)\n",
    "\n",
    "The CDF tells us \"what fraction of a normal distribution is less than $x$\". This is the probability interpretation behind GELU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the normal CDF\n",
    "from scipy.stats import norm\n",
    "\n",
    "x = np.linspace(-4, 4, 200)\n",
    "phi = norm.pdf(x)  # PDF\n",
    "Phi = norm.cdf(x)  # CDF\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(x, phi, linewidth=2)\n",
    "axes[0].fill_between(x, phi, alpha=0.3)\n",
    "axes[0].set_title('Normal PDF φ(x)')\n",
    "axes[0].set_xlabel('x')\n",
    "\n",
    "axes[1].plot(x, Phi, linewidth=2)\n",
    "axes[1].axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Φ(0) = 0.5')\n",
    "axes[1].set_title('Normal CDF Φ(x) = P(X ≤ x)')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Φ(-2) = {norm.cdf(-2):.4f}  (only 2.3% of normal is below -2)\")\n",
    "print(f\"Φ(0)  = {norm.cdf(0):.4f}  (exactly half)\")\n",
    "print(f\"Φ(2)  = {norm.cdf(2):.4f}  (97.7% is below 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: ReLU (Rectified Linear Unit)\n",
    "\n",
    "### 1.1 Definition and Intuition\n",
    "\n",
    "$$\\text{ReLU}(x) = \\max(0, x) = \\begin{cases} x & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases}$$\n",
    "\n",
    "**What it does**: ReLU simply passes positive values through unchanged and replaces negative values with zero.\n",
    "\n",
    "**Why we need activation functions at all**:\n",
    "\n",
    "Without activations, a neural network is just a composition of linear functions:\n",
    "$$y = W_3(W_2(W_1 x + b_1) + b_2) + b_3 = W_{\\text{combined}} x + b_{\\text{combined}}$$\n",
    "\n",
    "No matter how many layers, it collapses to a single linear transformation! Activation functions add **nonlinearity**, allowing the network to learn complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ReLU\n",
    "x = np.linspace(-3, 3, 200)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, np.maximum(0, x), linewidth=3, label='ReLU(x)')\n",
    "plt.plot(x, x, '--', alpha=0.5, label='y = x (identity)')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('ReLU(x)')\n",
    "plt.title('ReLU: Passes positive, blocks negative')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Why ReLU Replaced Sigmoid/Tanh\n",
    "\n",
    "Before ReLU (around 2012), networks used sigmoid or tanh:\n",
    "\n",
    "| Function | Formula | Range | Max Derivative |\n",
    "|----------|---------|-------|----------------|\n",
    "| Sigmoid | $\\sigma(x) = \\frac{1}{1+e^{-x}}$ | $(0, 1)$ | 0.25 |\n",
    "| Tanh | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | $(-1, 1)$ | 1.0 |\n",
    "| ReLU | $\\max(0, x)$ | $[0, \\infty)$ | 1.0 |\n",
    "\n",
    "**The vanishing gradient problem**:\n",
    "\n",
    "In backpropagation, gradients multiply through layers. If each layer shrinks the gradient by a factor (like sigmoid's max 0.25), after $n$ layers:\n",
    "\n",
    "$$\\text{gradient} \\propto 0.25^n \\rightarrow 0$$\n",
    "\n",
    "For a 10-layer network: $0.25^{10} \\approx 0.000001$ — the gradient essentially vanishes!\n",
    "\n",
    "ReLU's gradient is exactly **1** for positive inputs, so gradients don't shrink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare activation functions and their gradients\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "# Functions\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "tanh_vals = np.tanh(x)\n",
    "relu_vals = np.maximum(0, x)\n",
    "\n",
    "# Derivatives\n",
    "sigmoid_deriv = sigmoid * (1 - sigmoid)\n",
    "tanh_deriv = 1 - tanh_vals**2\n",
    "relu_deriv = (x > 0).astype(float)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(x, sigmoid, label='Sigmoid', linewidth=2)\n",
    "axes[0].plot(x, tanh_vals, label='Tanh', linewidth=2)\n",
    "axes[0].plot(x, relu_vals, label='ReLU', linewidth=2)\n",
    "axes[0].set_title('Activation Functions')\n",
    "axes[0].set_ylim(-2, 4)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(x, sigmoid_deriv, label=\"Sigmoid' (max=0.25)\", linewidth=2)\n",
    "axes[1].plot(x, tanh_deriv, label=\"Tanh' (max=1.0)\", linewidth=2)\n",
    "axes[1].plot(x, relu_deriv, label=\"ReLU' (=1 for x>0)\", linewidth=2)\n",
    "axes[1].set_title('Derivatives (Gradient Magnitude)')\n",
    "axes[1].axhline(y=1, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observation: ReLU derivative is constant 1 for x > 0\")\n",
    "print(\"This prevents gradient vanishing in deep networks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 ReLU Backward Pass\n",
    "\n",
    "**The derivative of ReLU**:\n",
    "\n",
    "$$\\frac{d}{dx}\\text{ReLU}(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{if } x < 0 \\end{cases}$$\n",
    "\n",
    "At $x = 0$, ReLU has a \"kink\" (not differentiable). We define $\\text{ReLU}'(0) = 0$ by convention.\n",
    "\n",
    "**Implementing the backward pass**:\n",
    "\n",
    "In backpropagation, we receive an \"upstream gradient\" $\\frac{\\partial L}{\\partial y}$ (how the loss changes with respect to the output). We need to compute $\\frac{\\partial L}{\\partial x}$ (how the loss changes with respect to the input).\n",
    "\n",
    "By the chain rule:\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\text{ReLU}'(x)$$\n",
    "\n",
    "In code:\n",
    "```python\n",
    "def relu_backward(x, upstream_grad):\n",
    "    return upstream_grad * (x > 0)  # Mask: 1 where x > 0, else 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate ReLU backward pass step by step\n",
    "x = np.array([-2.0, -0.5, 0.0, 0.5, 2.0])\n",
    "upstream_grad = np.array([1.0, 1.0, 1.0, 1.0, 1.0])  # Assume gradient of 1 from above\n",
    "\n",
    "print(\"Step-by-step ReLU backward:\")\n",
    "print(f\"1. Input x:           {x}\")\n",
    "print(f\"2. ReLU(x):           {relu(x)}\")\n",
    "print(f\"3. Mask (x > 0):      {(x > 0).astype(float)}\")\n",
    "print(f\"4. Upstream gradient: {upstream_grad}\")\n",
    "print(f\"5. dx = upstream * mask: {upstream_grad * (x > 0)}\")\n",
    "print(\"\\nNote: Gradient is 'blocked' where x ≤ 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 The \"Dying ReLU\" Problem\n",
    "\n",
    "If a neuron's weights evolve such that the input is **always negative** (for all training examples), then:\n",
    "- Forward: output is always 0\n",
    "- Backward: gradient is always 0\n",
    "- Update: weights never change\n",
    "\n",
    "The neuron is \"dead\" — it will never activate again.\n",
    "\n",
    "**When this happens**:\n",
    "- Large negative bias\n",
    "- Large learning rate causing weights to overshoot\n",
    "- Poor initialization\n",
    "\n",
    "**Solutions**:\n",
    "1. **Careful initialization** (He init: $\\sigma = \\sqrt{2/n_{\\text{in}}}$)\n",
    "2. **Leaky ReLU**: $f(x) = \\max(0.01x, x)$ — small gradient for negative inputs\n",
    "3. **GELU**: Smooth, never fully zero gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "### 2.1 The Probabilistic Intuition\n",
    "\n",
    "GELU was introduced with a beautiful probabilistic motivation. Consider:\n",
    "\n",
    "**Question**: What if instead of a hard threshold at 0, we randomly decided whether to keep each input, with the probability depending on how \"positive\" the input is?\n",
    "\n",
    "Let's define a \"stochastic ReLU\":\n",
    "$$\\text{StochasticReLU}(x) = x \\cdot m, \\quad \\text{where } m \\sim \\text{Bernoulli}(p)$$\n",
    "\n",
    "If we set $p = \\Phi(x)$ (the probability that a standard normal random variable is less than $x$), then:\n",
    "- For large positive $x$: $\\Phi(x) \\approx 1$, so we almost always keep the input (like ReLU)\n",
    "- For large negative $x$: $\\Phi(x) \\approx 0$, so we almost always zero it (like ReLU)\n",
    "- For $x \\approx 0$: $\\Phi(0) = 0.5$, so we keep it with 50% probability\n",
    "\n",
    "**GELU is the expected value of this stochastic ReLU**:\n",
    "\n",
    "$$\\text{GELU}(x) = \\mathbb{E}[x \\cdot m] = x \\cdot \\mathbb{E}[m] = x \\cdot P(m = 1) = x \\cdot \\Phi(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the probabilistic interpretation\n",
    "from scipy.stats import norm\n",
    "\n",
    "x = np.linspace(-4, 4, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# P(keep) = Phi(x)\n",
    "axes[0].plot(x, norm.cdf(x), linewidth=2)\n",
    "axes[0].set_title('Probability of keeping input: Φ(x)')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('P(keep)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# GELU = x * Phi(x)\n",
    "gelu_exact = x * norm.cdf(x)\n",
    "axes[1].plot(x, gelu_exact, label='GELU = x · Φ(x)', linewidth=2)\n",
    "axes[1].plot(x, np.maximum(0, x), '--', alpha=0.5, label='ReLU')\n",
    "axes[1].set_title('GELU: Expected value of stochastic ReLU')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Difference from ReLU\n",
    "diff = gelu_exact - np.maximum(0, x)\n",
    "axes[2].plot(x, diff, linewidth=2)\n",
    "axes[2].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[2].set_title('GELU - ReLU: Smooth transition near 0')\n",
    "axes[2].set_xlabel('x')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The Exact Formula\n",
    "\n",
    "$$\\text{GELU}(x) = x \\cdot \\Phi(x) = x \\cdot \\frac{1}{2}\\left[1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right]$$\n",
    "\n",
    "where $\\text{erf}$ is the **error function**:\n",
    "\n",
    "$$\\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_0^x e^{-t^2} dt$$\n",
    "\n",
    "The error function doesn't have a closed-form solution — you need numerical methods or lookup tables to compute it, which is slow.\n",
    "\n",
    "### 2.3 The Tanh Approximation\n",
    "\n",
    "For efficiency, we use a tanh-based approximation:\n",
    "\n",
    "$$\\text{GELU}(x) \\approx \\frac{x}{2}\\left[1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}}\\left(x + 0.044715 x^3\\right)\\right)\\right]$$\n",
    "\n",
    "**Where does this come from?**\n",
    "\n",
    "The error function can be approximated as:\n",
    "$$\\text{erf}(x) \\approx \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} \\cdot x \\cdot (1 + ax^2)\\right)$$\n",
    "\n",
    "The constant $a = 0.044715$ was found by minimizing the approximation error numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare exact vs approximation\n",
    "from scipy.special import erf\n",
    "\n",
    "x = np.linspace(-4, 4, 200)\n",
    "\n",
    "# Exact GELU using erf\n",
    "gelu_exact = x * 0.5 * (1 + erf(x / np.sqrt(2)))\n",
    "\n",
    "# Tanh approximation\n",
    "c = np.sqrt(2 / np.pi)\n",
    "gelu_approx = 0.5 * x * (1 + np.tanh(c * (x + 0.044715 * x**3)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(x, gelu_exact, label='Exact (erf)', linewidth=2)\n",
    "axes[0].plot(x, gelu_approx, '--', label='Approx (tanh)', linewidth=2)\n",
    "axes[0].legend()\n",
    "axes[0].set_title('GELU: Exact vs Tanh Approximation')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(x, np.abs(gelu_exact - gelu_approx) * 1000, linewidth=2)\n",
    "axes[1].set_title('Approximation Error (×1000)')\n",
    "axes[1].set_ylabel('|exact - approx| × 1000')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Maximum approximation error: {np.abs(gelu_exact - gelu_approx).max():.6f}\")\n",
    "print(\"This error is negligible for neural network training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 GELU Backward Pass Derivation\n",
    "\n",
    "Now we derive the gradient of GELU. Let's work with the tanh approximation.\n",
    "\n",
    "**Define intermediate variables**:\n",
    "- Let $u = \\sqrt{\\frac{2}{\\pi}}(x + 0.044715 x^3)$\n",
    "- Let $t = \\tanh(u)$\n",
    "- Then $\\text{GELU}(x) = \\frac{x}{2}(1 + t)$\n",
    "\n",
    "**Step 1: Apply the product rule**\n",
    "\n",
    "Since $\\text{GELU}(x) = \\frac{x}{2} \\cdot (1 + t)$, where both $\\frac{x}{2}$ and $t$ depend on $x$:\n",
    "\n",
    "$$\\frac{d}{dx}\\text{GELU}(x) = \\frac{d}{dx}\\left(\\frac{x}{2}\\right) \\cdot (1 + t) + \\frac{x}{2} \\cdot \\frac{d}{dx}(1 + t)$$\n",
    "\n",
    "$$= \\frac{1}{2}(1 + t) + \\frac{x}{2} \\cdot \\frac{dt}{dx}$$\n",
    "\n",
    "**Step 2: Find $\\frac{dt}{dx}$ using chain rule**\n",
    "\n",
    "We have $t = \\tanh(u)$, so:\n",
    "$$\\frac{dt}{dx} = \\frac{dt}{du} \\cdot \\frac{du}{dx}$$\n",
    "\n",
    "We know $\\frac{dt}{du} = 1 - \\tanh^2(u) = 1 - t^2$ (the tanh derivative we reviewed earlier).\n",
    "\n",
    "For $\\frac{du}{dx}$, with $u = c(x + ax^3)$ where $c = \\sqrt{2/\\pi}$ and $a = 0.044715$:\n",
    "$$\\frac{du}{dx} = c(1 + 3ax^2) = \\sqrt{\\frac{2}{\\pi}}(1 + 0.134145 x^2)$$\n",
    "\n",
    "**Step 3: Combine everything**\n",
    "\n",
    "$$\\boxed{\\text{GELU}'(x) = \\frac{1}{2}(1 + t) + \\frac{x}{2}(1 - t^2) \\cdot \\sqrt{\\frac{2}{\\pi}}(1 + 0.134145 x^2)}$$\n",
    "\n",
    "where $t = \\tanh\\left(\\sqrt{\\frac{2}{\\pi}}(x + 0.044715 x^3)\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement GELU backward step by step\n",
    "def gelu_backward_detailed(x):\n",
    "    \"\"\"GELU derivative with detailed steps.\"\"\"\n",
    "    # Constants\n",
    "    c = np.sqrt(2.0 / np.pi)  # ≈ 0.7979\n",
    "    a = 0.044715\n",
    "    \n",
    "    # Step 1: Compute u\n",
    "    u = c * (x + a * x**3)\n",
    "    print(f\"u = √(2/π) · (x + 0.044715x³) = {u}\")\n",
    "    \n",
    "    # Step 2: Compute t = tanh(u)\n",
    "    t = np.tanh(u)\n",
    "    print(f\"t = tanh(u) = {t}\")\n",
    "    \n",
    "    # Step 3: Compute du/dx\n",
    "    du_dx = c * (1 + 3 * a * x**2)\n",
    "    print(f\"du/dx = √(2/π) · (1 + 0.134145x²) = {du_dx}\")\n",
    "    \n",
    "    # Step 4: Compute dt/dx = (1 - t²) · du/dx\n",
    "    dt_dx = (1 - t**2) * du_dx\n",
    "    print(f\"dt/dx = (1 - t²) · du/dx = {dt_dx}\")\n",
    "    \n",
    "    # Step 5: Apply product rule\n",
    "    # d/dx[x/2 · (1+t)] = 1/2 · (1+t) + x/2 · dt/dx\n",
    "    term1 = 0.5 * (1 + t)\n",
    "    term2 = 0.5 * x * dt_dx\n",
    "    print(f\"Term 1: (1/2)(1 + t) = {term1}\")\n",
    "    print(f\"Term 2: (x/2) · dt/dx = {term2}\")\n",
    "    \n",
    "    result = term1 + term2\n",
    "    print(f\"GELU'(x) = Term1 + Term2 = {result}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with a single value\n",
    "x_test = np.array([1.0])\n",
    "print(\"=\" * 50)\n",
    "print(f\"Computing GELU'({x_test[0]}) step by step:\")\n",
    "print(\"=\" * 50)\n",
    "grad = gelu_backward_detailed(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with numerical gradient\n",
    "def numerical_gradient(f, x, eps=1e-5):\n",
    "    \"\"\"Compute gradient numerically using central difference.\"\"\"\n",
    "    return (f(x + eps) - f(x - eps)) / (2 * eps)\n",
    "\n",
    "def gelu_forward(x):\n",
    "    c = np.sqrt(2.0 / np.pi)\n",
    "    return 0.5 * x * (1.0 + np.tanh(c * (x + 0.044715 * x**3)))\n",
    "\n",
    "def gelu_backward_formula(x):\n",
    "    c = np.sqrt(2.0 / np.pi)\n",
    "    u = c * (x + 0.044715 * x**3)\n",
    "    t = np.tanh(u)\n",
    "    du_dx = c * (1.0 + 3.0 * 0.044715 * x**2)\n",
    "    return 0.5 * (1.0 + t) + 0.5 * x * (1 - t**2) * du_dx\n",
    "\n",
    "# Test on multiple values\n",
    "x_test = np.array([-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0])\n",
    "\n",
    "analytical = gelu_backward_formula(x_test)\n",
    "numerical = numerical_gradient(gelu_forward, x_test)\n",
    "\n",
    "print(\"Verification: Analytical vs Numerical Gradient\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'x':>8} {'Analytical':>12} {'Numerical':>12} {'Error':>12}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(x_test)):\n",
    "    error = abs(analytical[i] - numerical[i])\n",
    "    print(f\"{x_test[i]:>8.1f} {analytical[i]:>12.6f} {numerical[i]:>12.6f} {error:>12.2e}\")\n",
    "\n",
    "print(f\"\\nMax error: {np.abs(analytical - numerical).max():.2e}\")\n",
    "print(\"✓ Our analytical gradient matches numerical gradient!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GELU and its derivative\n",
    "x = np.linspace(-4, 4, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Function comparison\n",
    "axes[0].plot(x, gelu_forward(x), label='GELU', linewidth=2)\n",
    "axes[0].plot(x, np.maximum(0, x), '--', label='ReLU', linewidth=2, alpha=0.7)\n",
    "axes[0].axhline(y=0, color='k', linestyle='-', alpha=0.2)\n",
    "axes[0].axvline(x=0, color='k', linestyle='-', alpha=0.2)\n",
    "axes[0].set_title('GELU vs ReLU')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Derivative comparison  \n",
    "axes[1].plot(x, gelu_backward_formula(x), label=\"GELU'\", linewidth=2)\n",
    "axes[1].plot(x, (x > 0).astype(float), '--', label=\"ReLU'\", linewidth=2, alpha=0.7)\n",
    "axes[1].axhline(y=0, color='k', linestyle='-', alpha=0.2)\n",
    "axes[1].axhline(y=1, color='k', linestyle='--', alpha=0.2)\n",
    "axes[1].set_title('Derivatives: GELU is smooth, ReLU has discontinuity')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(f\"• GELU'(-1) = {gelu_backward_formula(np.array([-1.0]))[0]:.4f} (non-zero!)\")\n",
    "print(f\"• ReLU'(-1) = 0 (dead neuron)\")\n",
    "print(\"• GELU has smooth gradient flow everywhere\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Why GELU Works Better Than ReLU\n",
    "\n",
    "### 3.1 No Dead Neurons\n",
    "\n",
    "GELU's gradient is **never exactly zero**. Even for negative inputs, there's a small but non-zero gradient:\n",
    "\n",
    "| $x$ | ReLU'($x$) | GELU'($x$) |\n",
    "|-----|------------|------------|\n",
    "| -3 | 0 | 0.0036 |\n",
    "| -2 | 0 | 0.0183 |\n",
    "| -1 | 0 | 0.0833 |\n",
    "| 0 | 0 | 0.5000 |\n",
    "| 1 | 1 | 0.9167 |\n",
    "| 2 | 1 | 0.9820 |\n",
    "\n",
    "This means neurons can always learn, even if they're currently outputting near-zero values.\n",
    "\n",
    "### 3.2 Implicit Regularization\n",
    "\n",
    "GELU can be seen as a smoothed version of dropout:\n",
    "- Dropout: Randomly zero out neurons with probability $p$\n",
    "- GELU: Smoothly scale neurons based on their magnitude\n",
    "\n",
    "Both encourage the network to not rely too heavily on any single feature.\n",
    "\n",
    "### 3.3 Better Gradient Flow\n",
    "\n",
    "The smooth transition around zero means gradients change gradually, not abruptly. This helps optimization find better minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify our implementation matches\n",
    "from ai_comps.activations import gelu, gelu_backward\n",
    "\n",
    "x_test = np.random.randn(3, 4).astype(np.float32)\n",
    "\n",
    "# Our implementation\n",
    "y_impl = gelu(x_test)\n",
    "dy_impl = gelu_backward(x_test)\n",
    "\n",
    "# Manual computation\n",
    "y_manual = gelu_forward(x_test)\n",
    "dy_manual = gelu_backward_formula(x_test)\n",
    "\n",
    "print(\"Verification against ai_comps implementation:\")\n",
    "print(f\"Forward pass matches: {np.allclose(y_impl, y_manual)}\")\n",
    "print(f\"Backward pass matches: {np.allclose(dy_impl, dy_manual)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### ReLU\n",
    "- **Forward**: $\\text{ReLU}(x) = \\max(0, x)$\n",
    "- **Backward**: $\\text{ReLU}'(x) = \\mathbf{1}_{x > 0}$\n",
    "- **Pros**: Fast, simple, no vanishing gradient for positive inputs\n",
    "- **Cons**: Dead neurons, discontinuous gradient\n",
    "\n",
    "### GELU\n",
    "- **Forward**: $\\text{GELU}(x) = \\frac{x}{2}\\left[1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}}(x + 0.044715x^3)\\right)\\right]$\n",
    "- **Backward**: $\\text{GELU}'(x) = \\frac{1}{2}(1+t) + \\frac{x}{2}(1-t^2)\\sqrt{\\frac{2}{\\pi}}(1 + 0.134145x^2)$\n",
    "- **Pros**: Smooth, no dead neurons, probabilistic interpretation\n",
    "- **Cons**: ~5× more computation\n",
    "\n",
    "### When to use which\n",
    "- **ReLU**: CNNs, simple feedforward networks, when speed matters\n",
    "- **GELU**: Transformers, BERT, GPT, modern language models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}