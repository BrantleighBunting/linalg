{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanisms: Mathematical Derivations\n",
    "\n",
    "This notebook derives the forward and backward passes for attention mechanisms used in transformers,\n",
    "providing complete mathematical proofs and implementation verification."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Glossary of Terms\n\n| Term | Definition |\n|------|------------|\n| **Attention** | A mechanism that computes weighted combinations of values based on query-key similarity. Allows the model to focus on relevant parts of the input. |\n| **Query (Q)** | A vector representing \"what am I looking for?\" Derived from the current position's representation. Shape: $(T, d)$ or $(B, h, T, d)$ for multi-head. |\n| **Key (K)** | A vector representing \"what do I contain?\" Used to compute compatibility with queries. Same shape as Q. |\n| **Value (V)** | A vector representing \"what information do I provide?\" The actual content that gets aggregated based on attention weights. Same shape as Q. |\n| **Attention scores** | Raw compatibility scores $S = QK^T$, measuring how well each query matches each key. Shape: $(T_q, T_{kv})$. |\n| **Attention weights/probabilities** | Normalized scores after softmax: $P = \\text{softmax}(S)$. Each row sums to 1, forming a probability distribution over keys. |\n| **Scaled dot-product** | Dividing attention scores by $\\sqrt{d}$ to prevent large values that saturate softmax. Critical for stable gradients. |\n| **Multi-head attention (MHA)** | Running multiple attention operations in parallel with different learned projections, then concatenating results. Allows learning diverse attention patterns. |\n| **Head** | One of $h$ parallel attention computations in MHA. Each head has dimension $d = D/h$. |\n| **Causal mask** | A lower-triangular mask that prevents positions from attending to future positions. Essential for autoregressive generation (GPT-style models). |\n| **Self-attention** | When Q, K, V all come from the same sequence. The sequence attends to itself. |\n| **Cross-attention** | When Q comes from one sequence and K, V from another (e.g., in encoder-decoder models). |\n| **Projection matrices** | Learned weight matrices $W^Q, W^K, W^V, W^O$ that transform inputs into queries, keys, values, and final outputs. |\n| **Softmax saturation** | When softmax inputs are very large, outputs approach one-hot vectors with near-zero gradients. The $\\sqrt{d}$ scaling prevents this. |\n| **Jacobian** | Matrix of all partial derivatives. For softmax: $J_{ij} = \\frac{\\partial p_i}{\\partial x_j}$. |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulas and Theorems\n",
    "\n",
    "### Core Attention Formulas\n",
    "\n",
    "| Formula | Description |\n",
    "|---------|-------------|\n",
    "| $\\text{softmax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$ | Softmax function |\n",
    "| $\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$ | Scaled dot-product attention |\n",
    "| $S = \\frac{QK^T}{\\sqrt{d_k}}$ | Attention scores (scaled) |\n",
    "| $P = \\text{softmax}(S)$ | Attention probabilities |\n",
    "| $O = PV$ | Attention output |\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "| Formula | Description |\n",
    "|---------|-------------|\n",
    "| $Q_i = XW^Q_i$ | Query projection for head $i$ |\n",
    "| $K_i = XW^K_i$ | Key projection for head $i$ |\n",
    "| $V_i = XW^V_i$ | Value projection for head $i$ |\n",
    "| $\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i)$ | Single head output |\n",
    "| $\\text{MHA}(X) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O$ | Multi-head attention |\n",
    "\n",
    "### Backward Pass Formulas\n",
    "\n",
    "| Formula | Description |\n",
    "|---------|-------------|\n",
    "| $\\frac{\\partial \\text{softmax}_i}{\\partial x_j} = \\text{softmax}_i (\\delta_{ij} - \\text{softmax}_j)$ | Softmax Jacobian |\n",
    "| $\\frac{\\partial L}{\\partial S} = P \\odot \\left(\\frac{\\partial L}{\\partial P} - \\mathbf{1} \\left(\\frac{\\partial L}{\\partial P} \\cdot P\\right)^T\\right)$ | Softmax backward (row-wise) |\n",
    "| $\\frac{\\partial L}{\\partial V} = P^T \\frac{\\partial L}{\\partial O}$ | Gradient w.r.t. values |\n",
    "| $\\frac{\\partial L}{\\partial Q} = \\frac{1}{\\sqrt{d_k}} \\frac{\\partial L}{\\partial S} K$ | Gradient w.r.t. queries |\n",
    "| $\\frac{\\partial L}{\\partial K} = \\frac{1}{\\sqrt{d_k}} \\left(\\frac{\\partial L}{\\partial S}\\right)^T Q$ | Gradient w.r.t. keys |\n",
    "\n",
    "### Causal Masking\n",
    "\n",
    "| Formula | Description |\n",
    "|---------|-------------|\n",
    "| $M_{ij} = \\begin{cases} 0 & \\text{if } i \\geq j \\\\ -\\infty & \\text{if } i < j \\end{cases}$ | Causal mask (lower triangular) |\n",
    "| $\\text{softmax}(S + M)$ | Masked attention scores |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This notebook assumes familiarity with:\n",
    "\n",
    "### 1. Matrix Multiplication and Transposes\n",
    "\n",
    "For matrices $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$:\n",
    "- $(AB)_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}$\n",
    "- $(AB)^T = B^T A^T$\n",
    "- $\\frac{\\partial (XW)}{\\partial X} = W^T$ (when computing gradient flow)\n",
    "- $\\frac{\\partial (XW)}{\\partial W} = X^T$ (when computing parameter gradients)\n",
    "\n",
    "### 2. The Chain Rule for Matrices\n",
    "\n",
    "If $L$ is a scalar loss and we have $Y = f(X)$ and $L = g(Y)$, then:\n",
    "$$\\frac{\\partial L}{\\partial X_{ij}} = \\sum_{k,l} \\frac{\\partial L}{\\partial Y_{kl}} \\frac{\\partial Y_{kl}}{\\partial X_{ij}}$$\n",
    "\n",
    "In practice, we express this using matrix operations. For $Y = XW$:\n",
    "$$\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} W^T$$\n",
    "\n",
    "### 3. Softmax Function\n",
    "\n",
    "The softmax function converts a vector of real numbers into a probability distribution:\n",
    "$$\\text{softmax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}$$\n",
    "\n",
    "Properties:\n",
    "- Output is always positive: $\\text{softmax}(\\mathbf{x})_i > 0$\n",
    "- Outputs sum to 1: $\\sum_i \\text{softmax}(\\mathbf{x})_i = 1$\n",
    "- Translation invariant: $\\text{softmax}(\\mathbf{x} + c) = \\text{softmax}(\\mathbf{x})$\n",
    "\n",
    "### 4. The Kronecker Delta\n",
    "\n",
    "$$\\delta_{ij} = \\begin{cases} 1 & \\text{if } i = j \\\\ 0 & \\text{if } i \\neq j \\end{cases}$$\n",
    "\n",
    "This notation is used in the softmax Jacobian derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Softmax Derivation\n",
    "\n",
    "Before deriving attention, we need to fully understand softmax, as it's the core nonlinearity in attention.\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "Given input vector $\\mathbf{x} = [x_1, x_2, ..., x_n]$:\n",
    "\n",
    "$$p_i = \\text{softmax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}$$\n",
    "\n",
    "For numerical stability, we subtract the maximum:\n",
    "\n",
    "$$p_i = \\frac{e^{x_i - \\max(\\mathbf{x})}}{\\sum_{j=1}^{n} e^{x_j - \\max(\\mathbf{x})}}$$\n",
    "\n",
    "This doesn't change the output (softmax is translation invariant) but prevents overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Numerically stable softmax along last axis.\"\"\"\n",
    "    z = x - x.max(axis=-1, keepdims=True)  # Subtract max for stability\n",
    "    e = np.exp(z)\n",
    "    return e / e.sum(axis=-1, keepdims=True)\n",
    "\n",
    "# Example\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "p = softmax(x)\n",
    "print(f\"Input:  {x}\")\n",
    "print(f\"Output: {p}\")\n",
    "print(f\"Sum:    {p.sum():.6f} (should be 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass: Deriving the Softmax Jacobian\n",
    "\n",
    "We need to compute $\\frac{\\partial p_i}{\\partial x_j}$ for all $i, j$.\n",
    "\n",
    "Let $S = \\sum_k e^{x_k}$. Then $p_i = \\frac{e^{x_i}}{S}$.\n",
    "\n",
    "**Case 1: When $i = j$ (diagonal elements)**\n",
    "\n",
    "Using the quotient rule $\\frac{d}{dx}\\frac{u}{v} = \\frac{u'v - uv'}{v^2}$:\n",
    "\n",
    "$$\\frac{\\partial p_i}{\\partial x_i} = \\frac{e^{x_i} \\cdot S - e^{x_i} \\cdot e^{x_i}}{S^2} = \\frac{e^{x_i}}{S} - \\frac{e^{x_i}}{S} \\cdot \\frac{e^{x_i}}{S} = p_i - p_i^2 = p_i(1 - p_i)$$\n",
    "\n",
    "**Case 2: When $i \\neq j$ (off-diagonal elements)**\n",
    "\n",
    "$$\\frac{\\partial p_i}{\\partial x_j} = \\frac{0 \\cdot S - e^{x_i} \\cdot e^{x_j}}{S^2} = -\\frac{e^{x_i}}{S} \\cdot \\frac{e^{x_j}}{S} = -p_i p_j$$\n",
    "\n",
    "**Combined using Kronecker delta:**\n",
    "\n",
    "$$\\frac{\\partial p_i}{\\partial x_j} = p_i(\\delta_{ij} - p_j)$$\n",
    "\n",
    "This can be written in matrix form as the Jacobian:\n",
    "$$J = \\text{diag}(\\mathbf{p}) - \\mathbf{p}\\mathbf{p}^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_jacobian(p):\n",
    "    \"\"\"Compute the full Jacobian matrix of softmax.\"\"\"\n",
    "    # J[i,j] = p[i] * (delta[i,j] - p[j])\n",
    "    # = diag(p) - p @ p.T\n",
    "    return np.diag(p) - np.outer(p, p)\n",
    "\n",
    "# Verify the Jacobian\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "p = softmax(x)\n",
    "J = softmax_jacobian(p)\n",
    "\n",
    "print(\"Softmax output p:\", p)\n",
    "print(\"\\nJacobian matrix:\")\n",
    "print(J)\n",
    "print(f\"\\nDiagonal (p_i * (1 - p_i)): {p * (1 - p)}\")\n",
    "print(f\"Row sums (should be ~0):    {J.sum(axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Softmax Backward\n",
    "\n",
    "Given upstream gradient $\\frac{\\partial L}{\\partial \\mathbf{p}}$, we need $\\frac{\\partial L}{\\partial \\mathbf{x}}$.\n",
    "\n",
    "By chain rule:\n",
    "$$\\frac{\\partial L}{\\partial x_j} = \\sum_i \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial x_j} = \\sum_i \\frac{\\partial L}{\\partial p_i} p_i (\\delta_{ij} - p_j)$$\n",
    "\n",
    "Expanding:\n",
    "$$\\frac{\\partial L}{\\partial x_j} = \\frac{\\partial L}{\\partial p_j} p_j - p_j \\sum_i \\frac{\\partial L}{\\partial p_i} p_i$$\n",
    "\n",
    "Let $\\mathbf{g} = \\frac{\\partial L}{\\partial \\mathbf{p}}$ (upstream gradient). Then:\n",
    "$$\\frac{\\partial L}{\\partial x_j} = g_j p_j - p_j \\sum_i g_i p_i = p_j \\left(g_j - \\sum_i g_i p_i\\right)$$\n",
    "\n",
    "In vector form:\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{x}} = \\mathbf{p} \\odot \\left(\\mathbf{g} - (\\mathbf{g} \\cdot \\mathbf{p}) \\mathbf{1}\\right)$$\n",
    "\n",
    "where $\\odot$ is element-wise multiplication and $\\mathbf{g} \\cdot \\mathbf{p}$ is the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(dp, p):\n",
    "    \"\"\"Efficient softmax backward pass.\n",
    "    \n",
    "    Args:\n",
    "        dp: Upstream gradient (same shape as p)\n",
    "        p: Softmax output from forward pass\n",
    "    \n",
    "    Returns:\n",
    "        dx: Gradient w.r.t. input x\n",
    "    \"\"\"\n",
    "    # dot = sum(dp * p) - the dot product\n",
    "    dot = (dp * p).sum(axis=-1, keepdims=True)\n",
    "    # dx = p * (dp - dot)\n",
    "    return p * (dp - dot)\n",
    "\n",
    "# Verify against Jacobian method\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "p = softmax(x)\n",
    "dp = np.array([0.5, -0.3, 0.2])  # Arbitrary upstream gradient\n",
    "\n",
    "# Method 1: Full Jacobian (inefficient but clear)\n",
    "J = softmax_jacobian(p)\n",
    "dx_jacobian = J.T @ dp\n",
    "\n",
    "# Method 2: Efficient formula\n",
    "dx_efficient = softmax_backward(dp, p)\n",
    "\n",
    "print(f\"Via Jacobian:  {dx_jacobian}\")\n",
    "print(f\"Via efficient: {dx_efficient}\")\n",
    "print(f\"Match: {np.allclose(dx_jacobian, dx_efficient)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Scaled Dot-Product Attention\n",
    "\n",
    "The fundamental attention operation introduced in \"Attention Is All You Need\" (Vaswani et al., 2017).\n",
    "\n",
    "### The Intuition\n",
    "\n",
    "Attention computes a weighted combination of **values** ($V$), where the weights are determined by how well **queries** ($Q$) match **keys** ($K$).\n",
    "\n",
    "- **Query ($Q$)**: \"What am I looking for?\"\n",
    "- **Key ($K$)**: \"What do I contain?\"\n",
    "- **Value ($V$)**: \"What information do I provide?\"\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "Given:\n",
    "- $Q \\in \\mathbb{R}^{T_q \\times d}$ (queries)\n",
    "- $K \\in \\mathbb{R}^{T_{kv} \\times d}$ (keys)\n",
    "- $V \\in \\mathbb{R}^{T_{kv} \\times d}$ (values)\n",
    "\n",
    "**Step 1: Compute attention scores**\n",
    "$$S = \\frac{QK^T}{\\sqrt{d}}$$\n",
    "\n",
    "Shape: $S \\in \\mathbb{R}^{T_q \\times T_{kv}}$\n",
    "\n",
    "The scaling by $\\sqrt{d}$ is crucial. Without it, dot products grow with dimension, pushing softmax into saturation where gradients vanish.\n",
    "\n",
    "**Step 2: Apply softmax to get attention probabilities**\n",
    "$$P = \\text{softmax}(S)$$\n",
    "\n",
    "Each row of $P$ sums to 1, representing how much attention each query pays to each key.\n",
    "\n",
    "**Step 3: Compute weighted combination of values**\n",
    "$$O = PV$$\n",
    "\n",
    "Shape: $O \\in \\mathbb{R}^{T_q \\times d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_forward(Q, K, V, mask=None):\n",
    "    \"\"\"Scaled dot-product attention forward pass.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries (T_q, d)\n",
    "        K: Keys (T_kv, d)\n",
    "        V: Values (T_kv, d)\n",
    "        mask: Optional additive mask (T_q, T_kv)\n",
    "    \n",
    "    Returns:\n",
    "        O: Output (T_q, d)\n",
    "        cache: (Q, K, V, P, d) for backward pass\n",
    "    \"\"\"\n",
    "    T_q, d = Q.shape\n",
    "    scale = 1.0 / np.sqrt(d)\n",
    "    \n",
    "    # Step 1: Scaled attention scores\n",
    "    S = scale * (Q @ K.T)  # (T_q, T_kv)\n",
    "    \n",
    "    # Optional: Apply mask\n",
    "    if mask is not None:\n",
    "        S = S + mask\n",
    "    \n",
    "    # Step 2: Softmax to get probabilities\n",
    "    P = softmax(S)  # (T_q, T_kv)\n",
    "    \n",
    "    # Step 3: Weighted combination of values\n",
    "    O = P @ V  # (T_q, d)\n",
    "    \n",
    "    return O, (Q, K, V, P, d)\n",
    "\n",
    "# Example\n",
    "np.random.seed(42)\n",
    "T, d = 4, 3\n",
    "Q = np.random.randn(T, d)\n",
    "K = np.random.randn(T, d)\n",
    "V = np.random.randn(T, d)\n",
    "\n",
    "O, cache = attention_forward(Q, K, V)\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"Output shape: {O.shape}\")\n",
    "print(f\"\\nAttention probabilities P (rows sum to 1):\")\n",
    "print(cache[3])\n",
    "print(f\"Row sums: {cache[3].sum(axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (Q, K, V, P, d) = attention_forward(Q, K, V)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Attention weights\n",
    "im = axes[0].imshow(P, cmap='Blues', vmin=0, vmax=1)\n",
    "axes[0].set_xlabel('Key position')\n",
    "axes[0].set_ylabel('Query position')\n",
    "axes[0].set_title('Attention Probabilities P')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Raw scores (before softmax)\n",
    "S = (Q @ K.T) / np.sqrt(d)\n",
    "im = axes[1].imshow(S, cmap='RdBu', vmin=-2, vmax=2)\n",
    "axes[1].set_xlabel('Key position')\n",
    "axes[1].set_ylabel('Query position')\n",
    "axes[1].set_title('Attention Scores S (before softmax)')\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Scale by $\\sqrt{d}$?\n",
    "\n",
    "Consider the dot product $q \\cdot k = \\sum_{i=1}^{d} q_i k_i$.\n",
    "\n",
    "If $q_i$ and $k_i$ are independent with mean 0 and variance 1:\n",
    "- $E[q_i k_i] = 0$ (product of independent zero-mean RVs)\n",
    "- $\\text{Var}(q_i k_i) = E[q_i^2 k_i^2] = E[q_i^2] E[k_i^2] = 1$\n",
    "\n",
    "By independence of the $d$ terms:\n",
    "$$\\text{Var}(q \\cdot k) = d \\cdot \\text{Var}(q_i k_i) = d$$\n",
    "\n",
    "So $\\text{Std}(q \\cdot k) = \\sqrt{d}$. As $d$ grows, dot products become larger.\n",
    "\n",
    "Large values push softmax into saturation:\n",
    "- $\\text{softmax}([10, 0, 0]) \\approx [1, 0, 0]$ (nearly one-hot)\n",
    "- Gradients of softmax near saturation are near zero!\n",
    "\n",
    "Scaling by $\\sqrt{d}$ normalizes the variance back to $O(1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the variance scaling\n",
    "dims = [8, 64, 512, 2048]\n",
    "n_samples = 10000\n",
    "\n",
    "print(\"Dimension | Unscaled Std | Scaled Std | Expected\")\n",
    "print(\"-\" * 50)\n",
    "for d in dims:\n",
    "    q = np.random.randn(n_samples, d)\n",
    "    k = np.random.randn(n_samples, d)\n",
    "    \n",
    "    dots = (q * k).sum(axis=1)  # Dot products\n",
    "    scaled_dots = dots / np.sqrt(d)\n",
    "    \n",
    "    print(f\"{d:^9} | {dots.std():^12.2f} | {scaled_dots.std():^10.2f} | {np.sqrt(d):^8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Attention Backward Pass\n",
    "\n",
    "Now we derive the gradients. Given upstream gradient $\\frac{\\partial L}{\\partial O}$, we need:\n",
    "- $\\frac{\\partial L}{\\partial Q}$\n",
    "- $\\frac{\\partial L}{\\partial K}$\n",
    "- $\\frac{\\partial L}{\\partial V}$\n",
    "\n",
    "### Step 1: Gradient w.r.t. V\n",
    "\n",
    "From $O = PV$, treating each row independently:\n",
    "$$O_i = \\sum_j P_{ij} V_j \\quad \\Rightarrow \\quad \\frac{\\partial O_i}{\\partial V_k} = P_{ik}$$\n",
    "\n",
    "By chain rule:\n",
    "$$\\frac{\\partial L}{\\partial V_k} = \\sum_i \\frac{\\partial L}{\\partial O_i} P_{ik} = \\sum_i P_{ik} \\frac{\\partial L}{\\partial O_i}$$\n",
    "\n",
    "In matrix form:\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial V} = P^T \\frac{\\partial L}{\\partial O}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Gradient w.r.t. P\n",
    "\n",
    "From $O = PV$:\n",
    "$$\\frac{\\partial L}{\\partial P_{ij}} = \\sum_k \\frac{\\partial L}{\\partial O_{ik}} \\frac{\\partial O_{ik}}{\\partial P_{ij}}$$\n",
    "\n",
    "Since $O_{ik} = \\sum_l P_{il} V_{lk}$, we have $\\frac{\\partial O_{ik}}{\\partial P_{ij}} = V_{jk}$.\n",
    "\n",
    "Therefore:\n",
    "$$\\frac{\\partial L}{\\partial P_{ij}} = \\sum_k \\frac{\\partial L}{\\partial O_{ik}} V_{jk}$$\n",
    "\n",
    "In matrix form:\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial P} = \\frac{\\partial L}{\\partial O} V^T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Gradient through Softmax\n",
    "\n",
    "Using our efficient softmax backward formula, for each row:\n",
    "$$\\frac{\\partial L}{\\partial S_i} = P_i \\odot \\left(\\frac{\\partial L}{\\partial P_i} - \\left(\\frac{\\partial L}{\\partial P_i} \\cdot P_i\\right) \\mathbf{1}\\right)$$\n",
    "\n",
    "In matrix form (applied row-wise):\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial S} = P \\odot \\left(\\frac{\\partial L}{\\partial P} - \\text{rowsum}\\left(\\frac{\\partial L}{\\partial P} \\odot P\\right)\\right)}$$\n",
    "\n",
    "where $\\text{rowsum}$ means summing along each row and broadcasting back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Gradient w.r.t. Q and K\n",
    "\n",
    "Recall $S = \\frac{1}{\\sqrt{d}} Q K^T$.\n",
    "\n",
    "For $Q$: $S_{ij} = \\frac{1}{\\sqrt{d}} \\sum_k Q_{ik} K_{jk}$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial Q_{ik}} = \\sum_{j} \\frac{\\partial L}{\\partial S_{ij}} \\frac{1}{\\sqrt{d}} K_{jk}$$\n",
    "\n",
    "In matrix form:\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial Q} = \\frac{1}{\\sqrt{d}} \\frac{\\partial L}{\\partial S} K}$$\n",
    "\n",
    "For $K$: By similar derivation (or using the symmetry of transposed multiplication):\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial K} = \\frac{1}{\\sqrt{d}} \\left(\\frac{\\partial L}{\\partial S}\\right)^T Q}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_backward(dO, cache):\n",
    "    \"\"\"Backward pass for scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        dO: Upstream gradient (T_q, d)\n",
    "        cache: (Q, K, V, P, d) from forward pass\n",
    "    \n",
    "    Returns:\n",
    "        dQ, dK, dV: Gradients w.r.t. Q, K, V\n",
    "    \"\"\"\n",
    "    Q, K, V, P, d = cache\n",
    "    scale = 1.0 / np.sqrt(d)\n",
    "    \n",
    "    # Step 1: dV = P.T @ dO\n",
    "    dV = P.T @ dO\n",
    "    \n",
    "    # Step 2: dP = dO @ V.T\n",
    "    dP = dO @ V.T\n",
    "    \n",
    "    # Step 3: Softmax backward (row-wise)\n",
    "    # dS = P * (dP - sum(dP * P, axis=-1, keepdims=True))\n",
    "    rowdot = (dP * P).sum(axis=-1, keepdims=True)\n",
    "    dS = P * (dP - rowdot)\n",
    "    \n",
    "    # Step 4: dQ and dK\n",
    "    dQ = scale * (dS @ K)\n",
    "    dK = scale * (dS.T @ Q)\n",
    "    \n",
    "    return dQ, dK, dV\n",
    "\n",
    "# Verify with numerical gradient\n",
    "np.random.seed(42)\n",
    "T, d = 4, 3\n",
    "Q = np.random.randn(T, d)\n",
    "K = np.random.randn(T, d)\n",
    "V = np.random.randn(T, d)\n",
    "\n",
    "# Forward\n",
    "O, cache = attention_forward(Q, K, V)\n",
    "\n",
    "# Backward with arbitrary upstream gradient\n",
    "dO = np.random.randn(*O.shape)\n",
    "dQ, dK, dV = attention_backward(dO, cache)\n",
    "\n",
    "print(\"Analytic gradients computed!\")\n",
    "print(f\"dQ shape: {dQ.shape}\")\n",
    "print(f\"dK shape: {dK.shape}\")\n",
    "print(f\"dV shape: {dV.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Gradient Verification\n",
    "\n",
    "We verify our derivation using finite differences:\n",
    "$$\\frac{\\partial L}{\\partial x} \\approx \\frac{f(x + \\epsilon) - f(x - \\epsilon)}{2\\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x, eps=1e-5):\n",
    "    \"\"\"Compute numerical gradient using central differences.\"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        old_val = x[idx]\n",
    "        \n",
    "        x[idx] = old_val + eps\n",
    "        fplus = f(x)\n",
    "        x[idx] = old_val - eps\n",
    "        fminus = f(x)\n",
    "        \n",
    "        grad[idx] = (fplus - fminus) / (2 * eps)\n",
    "        x[idx] = old_val\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "# Define loss function: L = sum(O * dO) (dot product with arbitrary vector)\n",
    "def loss_wrt_Q(Q_test):\n",
    "    O_test, _ = attention_forward(Q_test, K, V)\n",
    "    return (O_test * dO).sum()\n",
    "\n",
    "def loss_wrt_K(K_test):\n",
    "    O_test, _ = attention_forward(Q, K_test, V)\n",
    "    return (O_test * dO).sum()\n",
    "\n",
    "def loss_wrt_V(V_test):\n",
    "    O_test, _ = attention_forward(Q, K, V_test)\n",
    "    return (O_test * dO).sum()\n",
    "\n",
    "# Compute numerical gradients\n",
    "dQ_num = numerical_gradient(loss_wrt_Q, Q.copy())\n",
    "dK_num = numerical_gradient(loss_wrt_K, K.copy())\n",
    "dV_num = numerical_gradient(loss_wrt_V, V.copy())\n",
    "\n",
    "# Compare\n",
    "print(\"Gradient verification:\")\n",
    "print(f\"  dQ max error: {np.abs(dQ - dQ_num).max():.2e}\")\n",
    "print(f\"  dK max error: {np.abs(dK - dK_num).max():.2e}\")\n",
    "print(f\"  dV max error: {np.abs(dV - dV_num).max():.2e}\")\n",
    "print(f\"\\nAll gradients match: {np.allclose(dQ, dQ_num) and np.allclose(dK, dK_num) and np.allclose(dV, dV_num)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Causal Masking\n",
    "\n",
    "In autoregressive models (like GPT), each position can only attend to previous positions. This is enforced with a **causal mask**.\n",
    "\n",
    "### The Mask\n",
    "\n",
    "For a sequence of length $T$, the causal mask is:\n",
    "$$M_{ij} = \\begin{cases} 0 & \\text{if } j \\leq i \\text{ (can attend)} \\\\ -\\infty & \\text{if } j > i \\text{ (blocked)} \\end{cases}$$\n",
    "\n",
    "When we add this to scores before softmax:\n",
    "$$P_{ij} = \\text{softmax}(S_{ij} + M_{ij})$$\n",
    "\n",
    "The $-\\infty$ values become 0 after softmax (since $e^{-\\infty} = 0$).\n",
    "\n",
    "### Visual Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(seq_len, fill=-1e9):\n",
    "    \"\"\"Build causal (lower triangular) attention mask.\"\"\"\n",
    "    i = np.arange(seq_len)\n",
    "    # mask[i,j] = fill if j > i else 0\n",
    "    return np.where(i[:, None] >= i[None, :], 0.0, fill)\n",
    "\n",
    "T = 6\n",
    "mask = causal_mask(T)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Raw mask\n",
    "axes[0].imshow(mask, cmap='RdBu', vmin=-10, vmax=0)\n",
    "axes[0].set_title('Causal Mask\\n(0 = attend, -âˆž = block)')\n",
    "axes[0].set_xlabel('Key position j')\n",
    "axes[0].set_ylabel('Query position i')\n",
    "\n",
    "# Random scores\n",
    "np.random.seed(0)\n",
    "S = np.random.randn(T, T)\n",
    "axes[1].imshow(S, cmap='RdBu', vmin=-2, vmax=2)\n",
    "axes[1].set_title('Raw Attention Scores S')\n",
    "\n",
    "# After softmax with mask\n",
    "P = softmax(S + mask)\n",
    "axes[2].imshow(P, cmap='Blues', vmin=0, vmax=1)\n",
    "axes[2].set_title('Attention Probs (masked)\\nLower triangular only')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAttention probabilities (notice zeros above diagonal):\")\n",
    "print(np.round(P, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Multi-Head Attention\n",
    "\n",
    "Instead of performing a single attention function, Multi-Head Attention (MHA) runs $h$ attention \"heads\" in parallel, each with different learned projections.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "1. **Different representation subspaces**: Each head can learn different aspects (e.g., syntax vs. semantics)\n",
    "2. **More expressive**: Multiple attention patterns simultaneously\n",
    "3. **Computational efficiency**: Same total dimension split across heads\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Given input $X \\in \\mathbb{R}^{B \\times T \\times D}$ where $D = h \\times d$:\n",
    "\n",
    "**Step 1: Linear projections**\n",
    "$$Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V$$\n",
    "where $W^Q, W^K, W^V \\in \\mathbb{R}^{D \\times D}$\n",
    "\n",
    "**Step 2: Split into heads**\n",
    "Reshape $(B, T, D) \\to (B, h, T, d)$\n",
    "\n",
    "**Step 3: Parallel attention**\n",
    "Apply scaled dot-product attention to each head independently\n",
    "\n",
    "**Step 4: Concatenate and project**\n",
    "$$\\text{MHA}(X) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    \"\"\"Multi-Head Attention with full forward/backward implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, seed=42):\n",
    "        assert d_model % n_heads == 0\n",
    "        self.D = d_model\n",
    "        self.h = n_heads\n",
    "        self.d = d_model // n_heads\n",
    "        \n",
    "        # Initialize projections\n",
    "        rng = np.random.default_rng(seed)\n",
    "        scale = np.sqrt(2.0 / d_model)\n",
    "        self.Wq = rng.normal(0, scale, (d_model, d_model)).astype(np.float32)\n",
    "        self.Wk = rng.normal(0, scale, (d_model, d_model)).astype(np.float32)\n",
    "        self.Wv = rng.normal(0, scale, (d_model, d_model)).astype(np.float32)\n",
    "        self.Wo = rng.normal(0, scale, (d_model, d_model)).astype(np.float32)\n",
    "        \n",
    "        self._cache = None\n",
    "    \n",
    "    def split_heads(self, X):\n",
    "        \"\"\"(B, T, D) -> (B, h, T, d)\"\"\"\n",
    "        B, T, _ = X.shape\n",
    "        return X.reshape(B, T, self.h, self.d).transpose(0, 2, 1, 3)\n",
    "    \n",
    "    def combine_heads(self, X):\n",
    "        \"\"\"(B, h, T, d) -> (B, T, D)\"\"\"\n",
    "        B, h, T, d = X.shape\n",
    "        return X.transpose(0, 2, 1, 3).reshape(B, T, h * d)\n",
    "    \n",
    "    def forward(self, X, mask=None):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            X: Input (B, T, D)\n",
    "            mask: Optional mask (1, 1, T, T) or broadcastable\n",
    "        \n",
    "        Returns:\n",
    "            Y: Output (B, T, D)\n",
    "        \"\"\"\n",
    "        B, T, D = X.shape\n",
    "        \n",
    "        # Linear projections\n",
    "        Q_lin = X @ self.Wq  # (B, T, D)\n",
    "        K_lin = X @ self.Wk\n",
    "        V_lin = X @ self.Wv\n",
    "        \n",
    "        # Split heads\n",
    "        Q = self.split_heads(Q_lin)  # (B, h, T, d)\n",
    "        K = self.split_heads(K_lin)\n",
    "        V = self.split_heads(V_lin)\n",
    "        \n",
    "        # Attention scores\n",
    "        scale = 1.0 / np.sqrt(self.d)\n",
    "        S = scale * np.einsum('bhtd,bhsd->bhts', Q, K)  # (B, h, T, T)\n",
    "        \n",
    "        if mask is not None:\n",
    "            S = S + mask\n",
    "        \n",
    "        P = softmax(S)  # (B, h, T, T)\n",
    "        \n",
    "        # Weighted values\n",
    "        O = np.einsum('bhts,bhsd->bhtd', P, V)  # (B, h, T, d)\n",
    "        \n",
    "        # Combine heads and output projection\n",
    "        H = self.combine_heads(O)  # (B, T, D)\n",
    "        Y = H @ self.Wo\n",
    "        \n",
    "        self._cache = (X, Q, K, V, P, H)\n",
    "        return Y\n",
    "    \n",
    "    def backward(self, dY):\n",
    "        \"\"\"Backward pass - returns dX and computes parameter gradients.\"\"\"\n",
    "        X, Q, K, V, P, H = self._cache\n",
    "        B, T, D = X.shape\n",
    "        scale = 1.0 / np.sqrt(self.d)\n",
    "        \n",
    "        # Output projection backward\n",
    "        dWo = H.reshape(-1, D).T @ dY.reshape(-1, D)\n",
    "        dH = dY @ self.Wo.T  # (B, T, D)\n",
    "        \n",
    "        # Split dH back to heads\n",
    "        dO = self.split_heads(dH)  # (B, h, T, d)\n",
    "        \n",
    "        # Attention backward\n",
    "        dV = np.einsum('bhts,bhtd->bhsd', P, dO)  # (B, h, T, d)\n",
    "        dP = np.einsum('bhtd,bhsd->bhts', dO, V)  # (B, h, T, T)\n",
    "        \n",
    "        # Softmax backward (row-wise on last two dims)\n",
    "        rowdot = (dP * P).sum(axis=-1, keepdims=True)\n",
    "        dS = P * (dP - rowdot)\n",
    "        \n",
    "        # Q, K backward\n",
    "        dQ = scale * np.einsum('bhts,bhsd->bhtd', dS, K)\n",
    "        dK = scale * np.einsum('bhts,bhtd->bhsd', dS, Q)\n",
    "        \n",
    "        # Combine heads for projection backward\n",
    "        dQ_lin = self.combine_heads(dQ)\n",
    "        dK_lin = self.combine_heads(dK)\n",
    "        dV_lin = self.combine_heads(dV)\n",
    "        \n",
    "        # Projection gradients\n",
    "        Xf = X.reshape(-1, D)\n",
    "        dWq = Xf.T @ dQ_lin.reshape(-1, D)\n",
    "        dWk = Xf.T @ dK_lin.reshape(-1, D)\n",
    "        dWv = Xf.T @ dV_lin.reshape(-1, D)\n",
    "        \n",
    "        # Input gradient\n",
    "        dX = (dQ_lin @ self.Wq.T + dK_lin @ self.Wk.T + dV_lin @ self.Wv.T)\n",
    "        \n",
    "        # Store gradients\n",
    "        self.dWq, self.dWk, self.dWv, self.dWo = dWq, dWk, dWv, dWo\n",
    "        \n",
    "        return dX\n",
    "\n",
    "# Test the implementation\n",
    "np.random.seed(42)\n",
    "B, T, D = 2, 5, 12\n",
    "n_heads = 3\n",
    "\n",
    "mha = MultiHeadAttention(D, n_heads)\n",
    "X = np.random.randn(B, T, D).astype(np.float32)\n",
    "mask = causal_mask(T)[None, None, :, :]  # (1, 1, T, T)\n",
    "\n",
    "Y = mha.forward(X, mask)\n",
    "print(f\"Input shape:  {X.shape}\")\n",
    "print(f\"Output shape: {Y.shape}\")\n",
    "print(f\"\\nNumber of heads: {n_heads}\")\n",
    "print(f\"Dimension per head: {D // n_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Multi-Head Attention Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical gradient check for MHA\n",
    "dY = np.random.randn(B, T, D).astype(np.float32)\n",
    "dX = mha.backward(dY)\n",
    "\n",
    "def loss_mha(X_test):\n",
    "    Y_test = mha.forward(X_test, mask)\n",
    "    return (Y_test * dY).sum()\n",
    "\n",
    "# Numerical gradient for X\n",
    "eps = 1e-4\n",
    "dX_num = np.zeros_like(X)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        for d in range(D):\n",
    "            X_plus = X.copy()\n",
    "            X_plus[b, t, d] += eps\n",
    "            X_minus = X.copy()\n",
    "            X_minus[b, t, d] -= eps\n",
    "            dX_num[b, t, d] = (loss_mha(X_plus) - loss_mha(X_minus)) / (2 * eps)\n",
    "\n",
    "print(\"Multi-Head Attention gradient check:\")\n",
    "print(f\"  dX max error: {np.abs(dX - dX_num).max():.2e}\")\n",
    "print(f\"  dX matches: {np.allclose(dX, dX_num, rtol=1e-3, atol=1e-5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Summary and Key Insights\n",
    "\n",
    "### What We Derived\n",
    "\n",
    "1. **Softmax backward**: $\\frac{\\partial L}{\\partial x} = p \\odot (g - (g \\cdot p))$\n",
    "\n",
    "2. **Scaled dot-product attention**:\n",
    "   - Forward: $O = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right) V$\n",
    "   - Backward: Chain rule through softmax, matrix multiplies\n",
    "\n",
    "3. **Multi-head attention**: Same operations, just with head dimension management\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Scaling matters**: Without $\\sqrt{d}$ scaling, attention scores explode with dimension\n",
    "\n",
    "2. **Softmax is row-wise**: Each query has its own probability distribution over keys\n",
    "\n",
    "3. **Gradients flow through both Q and K**: Changes to queries AND keys affect attention\n",
    "\n",
    "4. **Multi-head gives expressivity**: Different heads can learn different attention patterns\n",
    "\n",
    "### Computational Complexity\n",
    "\n",
    "For sequence length $T$ and dimension $D$:\n",
    "- Attention: $O(T^2 D)$ - quadratic in sequence length!\n",
    "- This is why long sequences are expensive (motivation for Flash Attention, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention heads learning different patterns\n",
    "np.random.seed(123)\n",
    "B, T, D = 1, 8, 16\n",
    "n_heads = 4\n",
    "\n",
    "mha = MultiHeadAttention(D, n_heads, seed=42)\n",
    "X = np.random.randn(B, T, D).astype(np.float32)\n",
    "\n",
    "_ = mha.forward(X, mask=causal_mask(T)[None, None, :, :])\n",
    "_, _, _, _, P, _ = mha._cache  # Get attention patterns\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
    "for h in range(4):\n",
    "    im = axes[h].imshow(P[0, h], cmap='Blues', vmin=0, vmax=1)\n",
    "    axes[h].set_title(f'Head {h+1}')\n",
    "    axes[h].set_xlabel('Key position')\n",
    "    if h == 0:\n",
    "        axes[h].set_ylabel('Query position')\n",
    "plt.colorbar(im, ax=axes[-1])\n",
    "plt.suptitle('Attention patterns per head (with causal mask)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}